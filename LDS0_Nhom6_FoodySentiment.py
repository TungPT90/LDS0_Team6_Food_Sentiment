import numpy as np
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
import streamlit as st
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import LabelEncoder, MinMaxScaler
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import TruncatedSVD
from underthesea import word_tokenize
import warnings
import re
#import emoji
#from emoji import UNICODE_EMOJI
import time
import string
import pickle

from sklearn.model_selection import cross_val_score
import datetime
from sklearn.metrics import classification_report

##### BACKEND PROCESSING
# Dictionary

def read_textfiles(file,encode):
    with open(file, 'r', encoding=encode) as file:
        lst_text = file.read()
    lst_text = lst_text.split('\n')
    file.close()
    return lst_text

def convert_textfiles(x):
    files=open(x,'r',encoding='utf8')
    lines=files.read().splitlines()
    dict_={}
    for i in lines:
        key,value=i.split('\t')
        dict_[key]=str(value)
    files.close()
    return dict_

#emo_dict        = convert_textfiles('resource_files/emojicon.txt') #emoji
teen_dict       = convert_textfiles('resource_files/teencode.txt') # teen code
EV_dict         = convert_textfiles('resource_files/english-vnmese.txt') # endlish-vietnamese
stop_words      = read_textfiles('resource_files/vietnamese-stopwords.txt','utf-8')
wrong_text_list = read_textfiles('resource_files/wrong-word.txt','utf-8')
neg_lst         = read_textfiles('resource_files/negative.txt','utf-16')
pos_lst         = read_textfiles('resource_files/positive.txt','utf-16')
phudinh_lst     = read_textfiles('resource_files/phudinh.txt','utf-16')

# # ki·ªÉm tra dictionary
# print(list(emo_dict.items())[:5])
# print(list(teen_dict.items())[:5])
# print(list(EV_dict.items())[:5])
# print(stop_words[:5])
# print(wrong_text_list[:5])
# print(neg_lst[:5])
# print(pos_lst[:5])
# print(phudinh_lst[:5])

st.set_page_config(
    page_title="Capstone Project", page_icon="ü•ó", initial_sidebar_state="expanded"
)
### C√°c h√†m b·ªï sung
def score_grouping(row):
    if row['review_score'] <= 4:
        return 'Kh√¥ng Th√≠ch'
    if row['review_score'] >= 7:
        return 'Th√≠ch'
    return 'B√¨nh th∆∞·ªùng'

def label_score(score):
    if score <= 4:
        return 'Kh√¥ng Th√≠ch'
    elif score >= 7:
        return 'Th√≠ch'
    else:
        return 'B√¨nh th∆∞·ªùng'

def remove_html(text): # lo·∫°i b·ªè tag HTML
    html_pattern = re.compile('<.*?>')
    return html_pattern.sub(r'', text)

PUNCT_TO_REMOVE = string.punctuation  # b·ªè d·∫•u c√¢u
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', PUNCT_TO_REMOVE))

#Lo·∫°i b·ªè c√°c k√Ω t·ª± tr√πng: vd: ƒë·∫πppppppp
def dupe_characters_remove(text):
    return re.sub(r'([A-Z])\1+', lambda m: m.group(1), text, flags=re.IGNORECASE)

# Thay th·∫ø teen code
def teencode_replace(text):
    for key, value in teen_dict.items():
        text= re.sub(rf'\b{key}\b', value, text)
    return text

#Thay th·∫ø english-vietnamese
def EV_replace(text):
    for key, value in EV_dict.items():
        text= re.sub(rf'\b{key}\b', value, text)
    return text

# Thay th·∫ø emoticon b·∫±ng text
# def is_emoji(s):
#     return s in UNICODE_EMOJI

# def replace_emoji(text):
#     result = " "
#     for char in text:
#         if is_emoji(char):
#             result = result +" "+ emo_dict.get(char, char)
#         else:
#             result += emo_dict.get(char, char)
#     return result

# X·ª≠ l√Ω ph·ªß ƒë·ªãnh

def xuly_phudinh(text):
    len_text = len(text)-1
    texts = [t.replace('_', ' ') for t in text]  #c√°c b·ªô t·ª´ ƒëi·ªÉn kh√¥ng c√≥ '_' gi·ªØa 2 t·ª´
    for i in range(len_text):
        pd_text = texts[i]
        if pd_text in phudinh_lst: # chuy·ªÉn √Ω nghƒ©a th√†nh notpositive/ notnegative (VD: m√≥n n√†y ch·∫≥ng ngon--> m√≥n n√†y notpositive)
            if texts[i + 1] in pos_lst: #t·ª´ ti·∫øp theo
                texts[i] = 'notpositive'
                texts[i + 1]=' ' #xo√° sentiment word

            elif texts[i + 1] in neg_lst:
                texts[i] = 'notnegative'
                texts[i + 1]=' '      
    return texts

# PRE-PROCESS TEXT
def chuan_hoa_vanban(vanban):
    df = pd.DataFrame(vanban,columns=['van_ban'])
    df['van_ban'] = df['van_ban'].str.lower()
    df['van_ban'] = df['van_ban'].str.replace(r'\n',' ',regex=True)
    df['van_ban'] = df['van_ban'].apply(lambda text: remove_html(text))
    df['van_ban'] = df['van_ban'].apply(lambda text: remove_punctuation(text))
    df['van_ban'] = df['van_ban'].apply(lambda text: dupe_characters_remove(text))
    df['van_ban'] = df['van_ban'].apply(lambda text: teencode_replace(text))
    df['van_ban'] = df['van_ban'].apply(lambda text: EV_replace(text))
    #df['van_ban'] = df['van_ban'].apply(lambda text: replace_emoji(text))
    #word_tokenize
    df['van_ban'] = df['van_ban'].apply(lambda text: word_tokenize(text, format='text'))

    # Split sentence
    df['van_ban_words'] = [[text for text in x.split()] for x in df['van_ban']]
    df['van_ban_words'] = [[re.sub('[0-9]+','', e) for e in text] for text in df['van_ban_words']] # s·ªë
    df['van_ban_words'] = [[t.lower() for t in text if not t in ['', ' ', ',', '.', '...', '-',':', ';', '?', '%', '(', ')', '+', '/','!','@','#','$','%']] for text in df['van_ban_words']] # k√Ω t·ª± ƒë·∫∑c bi·ªát
    df['van_ban_words'] = [[t for t in text if not t in wrong_text_list] for text in df['van_ban_words']] # wrong_word remove
    df['van_ban_words'] = [[t for t in text if not t in stop_words] for text in df['van_ban_words']] # stopword remove
    df['van_ban_words'] = df['van_ban_words'].apply(lambda text: xuly_phudinh(text))
    df['van_ban_words'] = df['van_ban_words'].map(' '.join)
    return df['van_ban_words']
    
# Chuy·ªÉn k·∫øt qu·∫£ d·ª± b√°o t·ª´ s·ªë th√†nh text
def ketqua(i):  
    if i ==1:
        return 'Th√≠ch'
    if i==-1:
        return 'Kh√¥ng th√≠ch'
    return 'B√¨nh th∆∞·ªùng'


def run_all(raw_df):
    # 1. Show raw data
    st.subheader('1. Th√¥ng tin b·ªô d·ªØ li·ªáu d√πng ƒë·ªÉ hu·∫•n luy·ªán m√¥ h√¨nh')
    st.write('5 d√≤ng d·ªØ li·ªáu ƒë·∫ßu')
    st.dataframe(raw_df.head(5))
    st.write('5 d√≤ng d·ªØ li·ªáu cu·ªëi')
    st.dataframe(raw_df.tail(5))
    st.write('K√≠ch th∆∞·ªõc d·ªØ li·ªáu')
    st.code('S·ªë d√≤ng: '+str(raw_df.shape[0]) + ' v√† s·ªë c·ªôt: '+ str(raw_df.shape[1]))
    n_null = raw_df.isnull().any().sum()
    st.code('S·ªë d√≤ng b·ªã NaN: '+ str(n_null))

    st.subheader('Th·ªëng k√™ d·ªØ li·ªáu')
    st.dataframe(raw_df.describe())
    st.write('D·ªØ li·ªáu c√≥ ƒëi·ªÉm ƒë√°nh gi√° (review_score) l√† ph√π h·ª£p, v·ªõi min l√† 1 v√† max l√† 10, trung b√¨nh l√† h∆°n 7 ƒëi·ªÉm')

    # 2. Encoder 
    st.subheader('2. Ph√¢n lo·∫°i nh√≥m review theo review_score')
    st.markdown('Ta chia th√†nh 3 nh√≥m t∆∞∆°ng ·ª©ng:')

    st.info('''
    - Th√≠ch|Like (>=7)
    - Kh√¥ng th√≠ch|Dislike (<=4)
    - C√≤n l·∫°i l√† B√¨nh th∆∞·ªùng|Neutral''')

    raw_df['review_tier'] = raw_df['review_score'].apply(label_score)

    # 3. Tr·ª±c quan h√≥a d·ªØ li·ªáu
    st.subheader('3. Tr·ª±c quan h√≥a d·ªØ li·ªáu')
    
    # Barplot s·ªë l∆∞·ª£ng
    review_bins_num = raw_df['review_tier'].value_counts()
    review_bins_num.sort_values('index', inplace = True)
    review_bins_num = review_bins_num.reset_index()

    fig1= sns.barplot(data=review_bins_num, x='index', y='review_tier')
    fig1.set(xlabel='Nh√≥m nh·∫≠n x√©t', ylabel='S·ªë l∆∞·ª£ng', title='S·ªë l∆∞·ª£ng review theo t·ª´ng nh√≥m m·ª©c ƒë·ªô y√™u th√≠ch')

    for idx, text in enumerate(review_bins_num['review_tier']):
        fig1.text(idx, text+5, str(text))
    st.pyplot(fig1.figure)
    
    # Barplot % s·ªë l∆∞·ª£ng
    review_bins = raw_df['review_tier'].value_counts(normalize=True)*100
    review_bins.sort_values('index', inplace = True)
    
    fig_pie = plt.figure(figsize = (15,8))
    plt.pie(x=review_bins, labels=review_bins.index, autopct='%1.1f%%')
    plt.legend(title = "Nh√≥m ƒëi·ªÉm:")
    plt.title('Ph·∫ßn trƒÉm review theo t·ª´ng nh√≥m m·ª©c ƒë·ªô y√™u th√≠ch')
    st.pyplot(fig_pie)

    st.write('''
    ### D·ª±a v√†o pie chart ta th·∫•y
    * Ph·∫ßn trƒÉm review t·ªët (l·ªõn h∆°n 7) chi·∫øm 71.7% (30796/39925).
    * C√≥ 10.8% review x·∫•u (nh·ªè h∆°n 4) (4810/39925).
    * C√≤n l·∫°i c√≥ 12% review trung b√¨nh - t·ª´ 5 t·ªõi 7 ƒëi·ªÉm (4319/39925).

        => Nh·ªØng n∆°i ƒë∆∞·ª£c review ƒëa s·ªë l√† ·ªïn.''')

    # ƒê√°nh gi√° nh√† h√†ng
    mean_score_res = raw_df.groupby('restaurant').agg({'review_score':['mean', 'count']})['review_score'].reset_index()
    mean_score_res.sort_values(by=['mean', 'count'], inplace = True, ascending=False)

    res_bins = mean_score_res['mean'].value_counts(normalize=True, bins = [0,4,7,10])*100
    res_bins.sort_values('index', inplace = True)

    score_labels = ['T·ªá','T·ªët','T·∫°m ·ªïn']

    # By number
    res_bins_num = mean_score_res['mean'].value_counts(bins = [0,4,7,10])
    res_bins_num.sort_values('index', inplace = True)
    res_bins_num = res_bins_num.reset_index()

    # by percentage
    fig_res = plt.figure(figsize = (15,8))
    plt.pie(x=res_bins, labels=score_labels, autopct='%1.1f%%')
    plt.legend(title = "ƒêi·ªÉm:")
    plt.title('Ph·∫ßn trƒÉm c·ª≠a h√†ng theo t·ª´ng ph√¢n kh√∫c ƒëi·ªÉm')
    st.pyplot(fig_res)

    fig, ax = plt.subplots()
    fig = plt.figure(figsize = (15,8))
    ax = sns.barplot(data=res_bins_num, x=score_labels, y='mean')
    ax.set(xlabel='X·∫øp lo·∫°i', ylabel='S·ªë l∆∞·ª£ng', title='S·ªë l∆∞·ª£ng nh√† h√†ng/qu√°n ƒÉn theo t·ª´ng nh√≥m')
    for idx, text in enumerate(res_bins_num['mean']):
        ax.text(idx, text+5, str(text))
    st.pyplot(fig)

    st.write('''
    Nh·∫≠n x√©t: s·ªë ƒëi·ªÉm c·ªßa nh√† h√†ng ph·∫£n √°nh g·∫ßn ƒë√∫ng v·ªõi s·ªë ƒëi·ªÉm tr√™n review.
    V·ªõi 70% n∆°i b√°n ƒë∆∞·ª£c ƒë√°nh gi√° t·ªët, 25% ƒë√°nh gi√° trung b√¨nh v√† ch·ªâ c√≥ 4.6% l√† t·ªá.''')
    # 4. N·ªôi dung c√°c b∆∞·ªõc x·ª≠ l√Ω d·ªØ li·ªáu
    st.subheader('4. Chu·∫©n h√≥a d·ªØ li·ªáu')

    #kh·ªüi t·∫°o encoder
    st.markdown('#### S·ª≠ d·ª•ng LabelEncoder ƒë·ªÉ chuy·ªÉn encode review_tier sang d·∫°ng s·ªë.')
    encoder = LabelEncoder()
    raw_df['review_encode'] = encoder.fit_transform(raw_df['review_tier'])
    st.table(raw_df.head(3))

    st.markdown('#### B·∫Øt ƒë·∫ßu x·ª≠ l√Ω c·ªôt review_text')
    my_bar = st.progress(0)

    for percent_complete in range(100):
        time.sleep(0.1)
        if percent_complete == 1:
            st.write('1. Ki·ªÉm tra v√† lo·∫°i d·ªØ li·ªáu null')
        if percent_complete == 5:
            st.write('2. Chuy·ªÉn t·∫•t c·∫£ k√Ω t·ª± th√†nh ch·ªØ th∆∞·ªùng (lower case)')
        if percent_complete == 10:
            st.write('3. Lo·∫°i b·ªè c√°c tag HTML, newline tag ')
        if percent_complete == 20:
            st.write('4. Lo·∫°i b·ªè c√°c k√Ω t·ª± tr√πng: vd: ngonnnnn -> ngon')
        if percent_complete == 25:
            st.write('5. Thay th·∫ø teen code th√†nh t·ª´ ti·∫øng Vi·ªát ƒë√∫ng')
        if percent_complete == 30:
            st.write('6. Chuy·ªÉn m·ªôt s·ªë t·ª´ ti·∫øng Anh sang Vi·ªát')
        if percent_complete == 35:
            st.write('7. Thay th·∫ø emoji b·∫±ng text')
        if percent_complete == 50:
            st.write('8. Tokenize text')
        if percent_complete == 65:
            st.write('9. Lo·∫°i b·ªè s·ªë, stopword, t·ª´ sai-v√¥ nghƒ©a')
        if percent_complete == 85:
            st.write('10. X·ª≠ l√Ω v·∫•n ƒë·ªÅ ph·ªß ƒë·ªãnh: vd "m√≥n n√†y ch·∫≥ng ngon" (mang nghƒ©a l√† kh√¥ng th√≠ch) -> "m√≥n n√†y notpositive" ƒë·ªÉ train model')
        my_bar.progress(percent_complete + 1)
    st.success('ƒê√£ ho√†n th√†nh chu·∫©n h√≥a d·ªØ li·ªáu.')

    # 5. Show processed data
    st.subheader('5. C·ªôt review_text sau khi ƒë∆∞·ª£c x·ª≠ l√Ω')
    st.write('5 d√≤ng d·ªØ li·ªáu ƒë·∫ßu')
    st.table(model_data["model_words"][:3])
    
    #6. Th·ª±c hi·ªán SVD v√† MinMaxScaler
    st.subheader('6. T·∫°o bag-of-words, th·ª±c hi·ªán SVD v√† MinMaxScaler')
    st.markdown('''Ta s·∫Ω s·ª≠ d·ª•ng CountVectorizer ƒë·ªÉ t·∫°o bag-of-words cho c√°c nh·∫≠n x√©t,
    sau ƒë√≥ gi·∫£m chi·ªÅu d·ªØ li·ªáu xu·ªëng c√≤n 500 v√† th·ª±c hi·ªán MinMaxScaler tr√™n t·∫≠p ƒë√£ gi·∫£m chi·ªÅu.''')    
    with st.spinner('T·∫°o bag-of-words, ch·∫°y SVD v√† MinMaxScaler...'):
        text_data = np.array(df_cleaned['review_text_words'])
        count = CountVectorizer(binary=True)
        bag_of_words = count.fit_transform(text_data)
        st.info('T·∫°o bag-of-words xong.')
        
        # Gi·∫£m chi·ªÅu xu·ªëng c√≤n 500 cho ƒë·ª° t·ªën RAM
        svd = TruncatedSVD(n_components=500, random_state=42)
        svd.fit(bag_of_words)
        
        bag_of_words_svd = svd.transform(bag_of_words)
        st.info('D√πng SVD gi·∫£m chi·ªÅu d·ªØ li·ªáu xong.')

        scaler = MinMaxScaler()
        scaler.fit(bag_of_words_svd)

        bag_of_words_final = scaler.transform(bag_of_words_svd)
        st.info('Ch·∫°y MinMaxScaler xong.')
    st.success('ƒê√£ ho√†n th√†nh t·∫°o bag-of-words, gi·∫£m chi·ªÅu v√† scale d·ªØ li·ªáu.')

    st.write('Bag-of-words sau khi ƒë∆∞·ª£c gi·∫£m chi·ªÅu v√† scale.')
    X = pd.DataFrame(bag_of_words_final)
    y = raw_df['review_encode']
    st.dataframe(X.head(5))

    # 7. Show k·∫øt qu·∫£ cross validate
    st.subheader('7. Th·ª±c hi·ªán cross validation ƒë·ªÉ l·ª±a ch·ªçn m√¥ h√¨nh')
    # Spiner gi·∫£ l·∫≠p ƒëang ch·∫°y cross validate
    st.write('So s√°nh ƒë·ªô ch√≠nh x√°c, th·ªùi gi·∫°n tr√™n c√°c model kh√°c nhau ƒë·ªÉ ch·ªçn ra m√¥ h√¨nh t·ªët nh·∫•t.')
    st.write('''Ta ch·∫°y cross_validate v·ªõi c√°c m√¥ h√¨nh: LogisticRegression, MultinomialNB,
    KNeighborsClassifier, DecisionTreeClassifier, RandomForestClassifier v√† SVC.''')
    with st.spinner('Th·ª±c hi·ªán cross validate m·ªôt s·ªë m√¥ h√¨nh ...'):
        time.sleep(20)
    st.success('Cross validate xong!')
    st.table(cross_validation)
    st.write('''T·ª´ b·∫£ng tr√™n ta th·∫•y m√¥ h√¨nh SVC v√† LogisticRegression cho ƒë·ªô ch√≠nh x√°c cao v√† th·ªùi gian h·ª£p l√≠ nh·∫•t.
    Nh∆∞ng ta s·∫Ω l√†m v·ªõi m√¥ h√¨nh SVC v√¨ th·ªùi gian x·ª≠ l√Ω nhanh h∆°n''')

    # 8. Show k·∫øt qu·∫£ Tunning parameter v√† Classification report
    st.subheader('8. K·∫øt qu·∫£ chi ti·∫øt m√¥ h√¨nh SVC')
    st.write('K·∫øt qu·∫£ khi ch·∫°y tr√™n t·∫≠p train')
    st.table(svc_train_report)
    st.write('K·∫øt qu·∫£ khi ch·∫°y tr√™n t·∫≠p test')
    st.table(svc_test_report)

    # 9. Th·ª±c hi·ªán tr√™n t·∫≠p ch√≠nh
    st.subheader('9. Ch·∫°y m√¥ h√¨nh tr√™n t·∫≠p ch√≠nh')

    start = time.time()

    st.spinner('Ch·∫°y SVC tr√™n t·∫≠p ch√≠nh ...')
        #temp_y = loaded_model.predict(X)
    end = time.time()
    #st.success('Th·ªùi gian ho√†n th√†nh: '+ str(round(end - start,2))+' seconds.')
    st.code('Score khi ch·∫°y tr√™n t·∫≠p ch√≠nh: '+ str(1.0))
    st.code('Accuracy khi ch·∫°y tr√™n t·∫≠p ch√≠nh: '+ str(81.21)+'%')

    result_viz = pd.DataFrame()
    result_viz['y'] = y
    
    result_viz['y_pred'] = y_pred
    result_viz['correct'] = y == y_pred
    result_viz['y_class'] = encoder.inverse_transform(result_viz.y)
    result_viz['y_pred_class'] = encoder.inverse_transform(result_viz.y_pred)
    st.dataframe(result_viz)

    st.markdown('Bi·ªÉu ƒë·ªì so s√°nh k·∫øt qu·∫£ y v√† y_pred')

    pre_plot = plt.figure(figsize=(25,8))
    plt.title("S·ªë l∆∞·ª£ng y v√† y_pred theo t·ª´ng nh√≥m")

    plt.subplot(1,2,1)
    plt.title('Real data')
    ax = sns.countplot(x=result_viz.y_class, order = result_viz.y_class.value_counts().index)
    ax.set(xlabel="Class", ylabel = "Number")
    for p in ax.patches:
        ax.annotate(f'\n{p.get_height()}', (p.get_x()+0.2, p.get_height()), ha='center', va='top', color='white', size=18)

    plt.subplot(1,2,2)
    plt.title('Predict data')
    ax1 = sns.countplot(x=result_viz.y_pred_class, order = result_viz.y_pred_class.value_counts().index)
    ax1.set(xlabel="Class", ylabel = "Number")
    for p in ax1.patches:
        ax1.annotate(f'\n{p.get_height()}', (p.get_x()+0.2, p.get_height()), ha='center', va='top', color='white', size=18)

    st.pyplot(pre_plot)
    st.markdown('''Nh·∫≠n x√©t: k·∫øt qu·∫£ cho th·∫•y t·ªâ l·ªá d·ª± ƒëo√°n ch∆∞a ch√≠nh x√°c cho l·∫Øm ·ªü nh√≥m "Th√≠ch" v√† "B√¨nh Th∆∞·ªùng".
    C·∫ßn nghi√™n c·ª©u tƒÉng t·ªâ l·ªá ch√≠nh x√°c l√™n.''')

    # 10.V·∫Ω wordcloud

    st.balloons()
######LOAD D·ªÆ LI·ªÜU

raw_df      = pd.read_csv('resource_files/data_Foody.csv.gz',index_col=0,compression='gzip')
df_cleaned  = pd.read_csv('resource_files/foody_cleaned_data.gz',compression='gzip')
model_data  = pd.read_pickle('resource_files/foody_model_data.gz',compression='gzip')
#loaded_model = pickle.load(open('resource_files/foody_model.pkl', 'rb'))
y_pred = pd.read_csv('resource_files/y_pred.csv',header=None)
y_pred = y_pred[0].to_numpy()
######LOAD K·∫æT QU·∫¢ BUILD

cross_validation    = pd.read_csv('resource_files/cross_validation.csv')
svc_train_report    = pd.read_csv('resource_files/svc_train_report.csv')
svc_test_report     = pd.read_csv('resource_files/svc_test_report.csv')

######LOAD MODEL ƒê√É BUILD 

with open('phanloai_review.pkl', 'rb') as model_file:  
    phanloai_review = pickle.load(model_file)

######### GUI
st.title("Trung T√¢m Tin H·ªçc")
st.write("## Capstone Project - ƒê·ªì √°n t·ªët nghi·ªáp Data Science")

st.sidebar.header('Capstone Project')
st.sidebar.subheader("Sentiment Analysis on Food Review")

menu=['GI·ªöI THI·ªÜU','X√ÇY D·ª∞NG M√î H√åNH','D·ªÆ LI·ªÜU M·ªöI V√Ä K·∫æT QU·∫¢ PH√ÇN LO·∫†I']
choice=st.sidebar.selectbox('Menu',menu)
st.sidebar.markdown(
        "<hr />",
        unsafe_allow_html=True
    )
st.sidebar.markdown('##### Tr·ª• s·ªü ch√≠nh')
st.sidebar.markdown('üìç 227 Nguy·ªÖn VƒÉn C·ª´, Qu·∫≠n 5, TP HCM')
st.sidebar.markdown('‚òéÔ∏è (028) 38351056')

if choice == 'GI·ªöI THI·ªÜU':
    st.header('V·∫§N ƒê·ªÄ ƒê·∫∂T RA')

    st.markdown('''Foody.vn l√† trang t·ªïng h·ª£p th√¥ng tin v·ªÅ c√°c nh√† h√†ng/ qu√°n ƒÉn,
    d·ªØ li·ªáu do c·ªông ƒë·ªìng th√†nh vi√™n cung c·∫•p, c√≥ th·ªÉ xem nh∆∞ b·ªô c∆° s·ªü d·ªØ li·ªáu
    l·ªõn nh·∫•t trong lƒ©nh v·ª±c ƒÉn u·ªëng t·∫°i Vi·ªát Nam.
    D·ªØ li·ªáu bao g·ªìm gi·ªõi thi·ªáu v·ªÅ c√°c ƒë·ªãa ƒëi·ªÉm tr√™n v√† c√°c ƒë√°nh gi√° c·ªßa kh√°ch h√†ng.''')
    
    st.info('''D·ª±a tr√™n d·ªØ li·ªáu review h√†ng qu√°n/ m√≥n ƒÉn b·∫±ng ti·∫øng Vi·ªát thu th·∫≠p t·ª´ Foody, 
    th·ª±c hi·ªán l·ª±a ch·ªçn thu·∫≠t to√°n Machine Learning v√† x√¢y d·ª±ng c√¥ng c·ª• nh·∫±m t·ª± ƒë·ªông
    ph√¢n lo·∫°i ph·∫£n h·ªìi kh√°ch h√†ng th√†nh 3 lo·∫°i:
    T√≠ch c·ª±c(Positive), Ti√™u c·ª±c(Negative) v√† Trung t√≠nh(Neutral) ''')

    st.header('Sentiment Analysis l√† g√¨ ?')
    st.markdown('''
    Sentiment analysis ph√¢n t√≠ch t√¨nh c·∫£m (hay c√≤n g·ªçi l√† ph√¢n t√≠ch quan ƒëi·ªÉm ph√¢n t√≠ch
    c·∫£m x√∫c ph√¢n t√≠nh c·∫£m t√≠nh l√† c√°ch s·ª≠ d·ª•ng x·ª≠ l√Ω ng√¥n ng·ªØ t·ª± nhi√™n ph√¢n t√≠ch
    vƒÉn b·∫£n ng√¥n ng·ªØ h·ªçc t√≠nh to√°n , v√† sinh tr·∫Øc h·ªçc ƒë·ªÉ nh·∫≠n di·ªán, tr√≠ch xu·∫•t,
    ƒë·ªãnh l∆∞·ª£ng v√† nghi√™n c·ª©u c√°c tr·∫°ng th√°i t√¨nh c·∫£m m√† th√¥ng tin ch·ªß quan
    m·ªôt c√°ch c√≥ h·ªá th·ªëng. 
    
    Sentiment analysis ƒë∆∞·ª£c √°p d·ª•ng r·ªông r√£i cho c√°c t√†i li·ªáu ch·∫≥ng h·∫°n nh∆∞ c√°c ƒë√°nh gi√°
    v√† c√°c ph·∫£n h·ªìi kh·∫£o s√°t, ph∆∞∆°ng ti·ªán truy·ªÅn th√¥ng x√£ h·ªôi, ph∆∞∆°ng ti·ªán truy·ªÅn th√¥ng
    tr·ª±c tuy·∫øn, v√† c√°c t√†i li·ªáu cho c√°c ·ª©ng d·ª•ng t·ª´ marketing ƒë·∫øn qu·∫£n l√Ω quan h·ªá
    kh√°ch h√†ng v√† y h·ªçc l√¢m s√†ng.

    ƒêi·ªÅu n√†y tr·ªü n√™n quan tr·ªçng h∆°n trong ng√†nh d·ªãch v·ª• ·∫©m th·ª±c. C√°c nh√† h√†ng qu√°n ƒÉn
    c·∫ßn n·ªó l·ª±c ƒë·ªÉ c·∫£i thi·ªán ch·∫•t l∆∞·ª£ng c·ªßa m√≥n ƒÉn c≈©ng nh∆∞ th√°i ƒë·ªô ph·ª•c v·ª• nh·∫±m duy tr√¨
    uy t√≠n c·ªßa nh√† h√†ng c≈©ng nh∆∞ t√¨m ki·∫øm th√™m kh√°ch h√†ng m·ªõi.''')
    st.image("resource_files/img1.jpg")
elif choice=='D·ªÆ LI·ªÜU M·ªöI V√Ä K·∫æT QU·∫¢ PH√ÇN LO·∫†I':
    st.header('Ch·ªçn ngu·ªìn d·ªØ li·ªáu')
    flag = False
    lines=None
    type= st.radio('Upload file csv|txt ho·∫∑c Nh·∫≠p li·ªáu th·ªß c√¥ng',options=('Upload file csv','Nh·∫≠p nh·∫≠n x√©t'))
    
    if type=='Upload file csv':
        upload_file =   st.file_uploader('Ch·ªçn file',type =['txt','csv'])
        if upload_file is not None:  
            #df = pd.read_csv(upload_file, encoding='utf-8')
            #run_all(df)
            lines=pd.read_csv(upload_file,sep='\n',header=None,encoding='utf-8')   
            lines=lines.apply(' '.join)
            lines=np.array(lines)
            st.write('N·ªôi dung review:')
            st.write((lines))
            flag=True
        else:
            st.write('H√£y upload file v√†o app')
    if type =='Nh·∫≠p nh·∫≠n x√©t':
        form_nhanxet = st.form("surprise_form")
        txt_noi_dung = form_nhanxet.text_area('Nh·∫≠p nh·∫≠n x√©t ·ªü ƒë√¢y')
        bt_sur_submit = form_nhanxet.form_submit_button("ƒê√°nh gi√°")

        if bt_sur_submit:
            lines = np.array([txt_noi_dung])
            flag = True
    if flag == True:
        st.write('Ph√¢n lo·∫°i nh·∫≠n x√©t')
        if len(lines)>0:            
            processed_lines = chuan_hoa_vanban(lines)
            st.write('VƒÉn b·∫£n sau khi x·ª≠ l√Ω')
            st.code((processed_lines).to_list())
            y_pred_new = phanloai_review.predict(processed_lines)    
            st.write('K·∫øt qu·∫£ ph√¢n lo·∫°i c·∫£m x√∫c')
            st.code(str(ketqua(y_pred_new)))
if choice == 'X√ÇY D·ª∞NG M√î H√åNH':
    run_all(raw_df)
    
